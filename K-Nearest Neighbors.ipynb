{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soru 1:\n",
    "Find a data set and build a KNN Regression and an OLS regression. Compare the two. How similar are they? Do they miss in different ways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': 'C:\\\\Users\\\\afran\\\\Anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\breast_cancer.csv'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer=load_breast_cancer()\n",
    "cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.9</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.8</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.5</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  ...  worst symmetry  worst fractal dimension  target\n",
       "0        17.99         10.38           122.8  ...          0.4601                  0.11890       0\n",
       "1        20.57         17.77           132.9  ...          0.2750                  0.08902       0\n",
       "2        19.69         21.25           130.0  ...          0.3613                  0.08758       0\n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df[\"target\"]=cancer.target\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=df.target\n",
    "X=df.iloc[:,:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6274165202108963\n",
      "0.37258347978910367\n"
     ]
    }
   ],
   "source": [
    "# Look at whether imbalanced data or not\n",
    "\n",
    "print(sum(df[\"target\"])/len(df[\"target\"]*100))\n",
    "print(1-(sum(df[\"target\"])/len(df[\"target\"]*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6285714285714286\n",
      "0.6228070175438597 \n",
      "\n",
      "169\n",
      "286\n",
      "455\n"
     ]
    }
   ],
   "source": [
    "print(sum(Y_train)/len(Y_train*100))\n",
    "print(sum(Y_test)/len(Y_test*100),\"\\n\")\n",
    "\n",
    "print(len(Y_train[Y_train==0]))\n",
    "print(len(Y_train[Y_train==1]))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg=linear_model.LogisticRegression()\n",
    "log_reg.fit(X_train,Y_train)\n",
    "\n",
    "y_train_pred=log_reg.predict(X_train)\n",
    "y_test_pred=log_reg.predict(X_test)\n",
    "\n",
    "log_reg.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set scores: [0.96043956 0.94505495 0.94505495 0.96263736 0.94736842]\n",
      "Mean of train score: 0.9521110468478889\n",
      "Test set scores: [0.92982456 0.93859649 0.96491228 0.95614035 0.95575221]\n",
      "Mean of test set scores: 0.9490451793199813\n"
     ]
    }
   ],
   "source": [
    "# Firstly, apply cross validation\n",
    "\n",
    "CV=cross_validate(estimator=log_reg, \n",
    "                  X=X, \n",
    "                  y=Y, \n",
    "                  cv=5,\n",
    "                 return_train_score=True)\n",
    "\n",
    "print(\"Train set scores:\",CV['train_score'])\n",
    "print(\"Mean of train score:\", CV[\"train_score\"].mean())\n",
    "print(\"Test set scores:\", CV[\"test_score\"])\n",
    "print(\"Mean of test set scores:\", CV[\"test_score\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>target</td>      <th>  R-squared:         </th> <td>   0.779</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.763</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   49.81</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 26 Sep 2020</td> <th>  Prob (F-statistic):</th> <td>2.23e-119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:53:43</td>     <th>  Log-Likelihood:    </th> <td>  28.738</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   455</td>      <th>  AIC:               </th> <td>   4.525</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   424</td>      <th>  BIC:               </th> <td>   132.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    30</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>                <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                   <td>    3.0563</td> <td>    0.476</td> <td>    6.420</td> <td> 0.000</td> <td>    2.121</td> <td>    3.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean radius</th>             <td>    0.1971</td> <td>    0.200</td> <td>    0.987</td> <td> 0.324</td> <td>   -0.195</td> <td>    0.590</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean texture</th>            <td>   -0.0028</td> <td>    0.009</td> <td>   -0.311</td> <td> 0.756</td> <td>   -0.020</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean perimeter</th>          <td>   -0.0228</td> <td>    0.029</td> <td>   -0.777</td> <td> 0.438</td> <td>   -0.080</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean area</th>               <td>   -0.0003</td> <td>    0.001</td> <td>   -0.545</td> <td> 0.586</td> <td>   -0.002</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean smoothness</th>         <td>    0.4115</td> <td>    2.236</td> <td>    0.184</td> <td> 0.854</td> <td>   -3.984</td> <td>    4.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean compactness</th>        <td>    5.0017</td> <td>    1.523</td> <td>    3.285</td> <td> 0.001</td> <td>    2.009</td> <td>    7.994</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean concavity</th>          <td>   -1.0059</td> <td>    1.165</td> <td>   -0.864</td> <td> 0.388</td> <td>   -3.295</td> <td>    1.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean concave points</th>     <td>   -4.9157</td> <td>    2.311</td> <td>   -2.127</td> <td> 0.034</td> <td>   -9.459</td> <td>   -0.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean symmetry</th>           <td>    0.3384</td> <td>    0.826</td> <td>    0.409</td> <td> 0.682</td> <td>   -1.286</td> <td>    1.963</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean fractal dimension</th>  <td>   -5.8143</td> <td>    6.306</td> <td>   -0.922</td> <td> 0.357</td> <td>  -18.209</td> <td>    6.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>radius error</th>            <td>   -0.4323</td> <td>    0.344</td> <td>   -1.257</td> <td> 0.209</td> <td>   -1.108</td> <td>    0.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>texture error</th>           <td>    0.0126</td> <td>    0.042</td> <td>    0.300</td> <td> 0.764</td> <td>   -0.070</td> <td>    0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>perimeter error</th>         <td>    0.0082</td> <td>    0.046</td> <td>    0.179</td> <td> 0.858</td> <td>   -0.082</td> <td>    0.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>area error</th>              <td>    0.0012</td> <td>    0.001</td> <td>    0.833</td> <td> 0.405</td> <td>   -0.002</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>smoothness error</th>        <td>  -18.0785</td> <td>    7.036</td> <td>   -2.569</td> <td> 0.011</td> <td>  -31.909</td> <td>   -4.248</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>compactness error</th>       <td>    2.2080</td> <td>    2.302</td> <td>    0.959</td> <td> 0.338</td> <td>   -2.317</td> <td>    6.733</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>concavity error</th>         <td>    4.2738</td> <td>    1.406</td> <td>    3.039</td> <td> 0.003</td> <td>    1.510</td> <td>    7.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>concave points error</th>    <td>  -18.1590</td> <td>    6.173</td> <td>   -2.942</td> <td> 0.003</td> <td>  -30.293</td> <td>   -6.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>symmetry error</th>          <td>    1.1945</td> <td>    3.113</td> <td>    0.384</td> <td> 0.701</td> <td>   -4.924</td> <td>    7.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fractal dimension error</th> <td>    3.0120</td> <td>   12.404</td> <td>    0.243</td> <td> 0.808</td> <td>  -21.369</td> <td>   27.393</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>worst radius</th>            <td>   -0.2144</td> <td>    0.065</td> <td>   -3.303</td> <td> 0.001</td> <td>   -0.342</td> <td>   -0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>worst texture</th>           <td>   -0.0096</td> <td>    0.008</td> <td>   -1.218</td> <td> 0.224</td> <td>   -0.025</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>worst perimeter</th>         <td>    0.0087</td> <td>    0.007</td> <td>    1.259</td> <td> 0.209</td> <td>   -0.005</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>worst area</th>              <td>    0.0010</td> <td>    0.000</td> <td>    2.629</td> <td> 0.009</td> <td>    0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>worst smoothness</th>        <td>   -0.1324</td> <td>    1.565</td> <td>   -0.085</td> <td> 0.933</td> <td>   -3.209</td> <td>    2.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>worst compactness</th>       <td>   -0.7627</td> <td>    0.431</td> <td>   -1.770</td> <td> 0.077</td> <td>   -1.610</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>worst concavity</th>         <td>   -0.6157</td> <td>    0.293</td> <td>   -2.104</td> <td> 0.036</td> <td>   -1.191</td> <td>   -0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>worst concave points</th>    <td>    1.3262</td> <td>    1.050</td> <td>    1.263</td> <td> 0.207</td> <td>   -0.738</td> <td>    3.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>worst symmetry</th>          <td>   -1.0211</td> <td>    0.541</td> <td>   -1.888</td> <td> 0.060</td> <td>   -2.084</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>worst fractal dimension</th> <td>   -1.2736</td> <td>    2.702</td> <td>   -0.471</td> <td> 0.638</td> <td>   -6.585</td> <td>    4.038</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>26.704</td> <th>  Durbin-Watson:     </th> <td>   2.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  29.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.620</td> <th>  Prob(JB):          </th> <td>3.26e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.198</td> <th>  Cond. No.          </th> <td>1.47e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.47e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 target   R-squared:                       0.779\n",
       "Model:                            OLS   Adj. R-squared:                  0.763\n",
       "Method:                 Least Squares   F-statistic:                     49.81\n",
       "Date:                Sat, 26 Sep 2020   Prob (F-statistic):          2.23e-119\n",
       "Time:                        09:53:43   Log-Likelihood:                 28.738\n",
       "No. Observations:                 455   AIC:                             4.525\n",
       "Df Residuals:                     424   BIC:                             132.3\n",
       "Df Model:                          30                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===========================================================================================\n",
       "                              coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------------------\n",
       "const                       3.0563      0.476      6.420      0.000       2.121       3.992\n",
       "mean radius                 0.1971      0.200      0.987      0.324      -0.195       0.590\n",
       "mean texture               -0.0028      0.009     -0.311      0.756      -0.020       0.015\n",
       "mean perimeter             -0.0228      0.029     -0.777      0.438      -0.080       0.035\n",
       "mean area                  -0.0003      0.001     -0.545      0.586      -0.002       0.001\n",
       "mean smoothness             0.4115      2.236      0.184      0.854      -3.984       4.806\n",
       "mean compactness            5.0017      1.523      3.285      0.001       2.009       7.994\n",
       "mean concavity             -1.0059      1.165     -0.864      0.388      -3.295       1.284\n",
       "mean concave points        -4.9157      2.311     -2.127      0.034      -9.459      -0.372\n",
       "mean symmetry               0.3384      0.826      0.409      0.682      -1.286       1.963\n",
       "mean fractal dimension     -5.8143      6.306     -0.922      0.357     -18.209       6.580\n",
       "radius error               -0.4323      0.344     -1.257      0.209      -1.108       0.244\n",
       "texture error               0.0126      0.042      0.300      0.764      -0.070       0.095\n",
       "perimeter error             0.0082      0.046      0.179      0.858      -0.082       0.099\n",
       "area error                  0.0012      0.001      0.833      0.405      -0.002       0.004\n",
       "smoothness error          -18.0785      7.036     -2.569      0.011     -31.909      -4.248\n",
       "compactness error           2.2080      2.302      0.959      0.338      -2.317       6.733\n",
       "concavity error             4.2738      1.406      3.039      0.003       1.510       7.038\n",
       "concave points error      -18.1590      6.173     -2.942      0.003     -30.293      -6.025\n",
       "symmetry error              1.1945      3.113      0.384      0.701      -4.924       7.313\n",
       "fractal dimension error     3.0120     12.404      0.243      0.808     -21.369      27.393\n",
       "worst radius               -0.2144      0.065     -3.303      0.001      -0.342      -0.087\n",
       "worst texture              -0.0096      0.008     -1.218      0.224      -0.025       0.006\n",
       "worst perimeter             0.0087      0.007      1.259      0.209      -0.005       0.022\n",
       "worst area                  0.0010      0.000      2.629      0.009       0.000       0.002\n",
       "worst smoothness           -0.1324      1.565     -0.085      0.933      -3.209       2.945\n",
       "worst compactness          -0.7627      0.431     -1.770      0.077      -1.610       0.084\n",
       "worst concavity            -0.6157      0.293     -2.104      0.036      -1.191      -0.040\n",
       "worst concave points        1.3262      1.050      1.263      0.207      -0.738       3.390\n",
       "worst symmetry             -1.0211      0.541     -1.888      0.060      -2.084       0.042\n",
       "worst fractal dimension    -1.2736      2.702     -0.471      0.638      -6.585       4.038\n",
       "==============================================================================\n",
       "Omnibus:                       26.704   Durbin-Watson:                   2.090\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               29.870\n",
       "Skew:                          -0.620   Prob(JB):                     3.26e-07\n",
       "Kurtosis:                       3.198   Cond. No.                     1.47e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.47e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=sm.add_constant(X_train)\n",
    "\n",
    "result=sm.OLS(Y_train,X_train).fit()\n",
    "result.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelin eğitim performansı (R-squared)= 0.779  Şimdi KNN uygulayalım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9298245614035088"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "knn.fit(X_train,Y_train)\n",
    "\n",
    "predict_knn=knn.score(X_test,Y_test)\n",
    "predict_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9490451793199813\n"
     ]
    }
   ],
   "source": [
    "cross=cross_validate(estimator=knn,\n",
    "                    X=X_test,\n",
    "                    y=Y_test,\n",
    "                    cv=3,\n",
    "                    return_train_score=True)\n",
    "\n",
    "print(cv[\"test_score\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Çapraz doğrulama sonrasında test datasının doğruluğu biraz düşmüştür. Logistic regresyon gibi sınıflandırma algoritmalarında OLS kullanılmaz.Çünkü zaten sonuç 1 ya da 0 olacağından hataların karelerinin toplamını en aza indirme gibi bir şey olmaz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Least Square regression(OLS) is not built for binary classification, as logistic regression performs a better job at classifying data points and has a better logarithmic loss function as opposed to least squares regression. KNN algorithms ise used in either regression or classification problems; but OLS is used only in regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soru 2:\n",
    "What are potential problems with implementing kNN on a very large dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In large datasets, the cost of calculating the distance between the new point and each existing points is huge which degrades the performance of the algorithm.\n",
    "* The KNN algorithm doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate the distance in each dimension.\n",
    "* We need to do feature scaling (standardization and normalization) before applying KNN algorithm to any dataset. If we don't do so, KNN may generate wrong predictions.\n",
    "* KNN is sensitive to noise in the dataset. We need to manually impute missing values and remove outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soru 3:\n",
    "Describe the training set and test set accuracy as the value of K increases?<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.9298245614035088\n",
      "Train set accuracy: 0.9648351648351648\n",
      "Test set accuracy by other way 0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "# If value of k increase, accuracy of the model increase. Let's test it:\n",
    "\n",
    "knn=KNeighborsClassifier(n_neighbors=2, metric=\"minkowski\")\n",
    "\n",
    "knn.fit(X_train,Y_train)\n",
    "\n",
    "result=knn.predict(X_test)\n",
    "\n",
    "score_knn_test=knn.score(X_test,Y_test)\n",
    "print(\"Test set accuracy: \",score_knn_test)\n",
    "\n",
    "score_knn_train=knn.score(X_train,Y_train)\n",
    "print(\"Train set accuracy:\", score_knn_train)\n",
    "\n",
    "\n",
    "# Accuracy değeri iki türlü bulunabilir: Ya direkt. score(X_train,Y_train) denilir; ya da accuracy_score(Y_test,y_test_pred)  denilebilir.\n",
    "print(\"Test set accuracy by other way\",accuracy_score(Y_test,result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test set for k=5 : 0.956140350877193\n",
      "Accuracy of train set for k=5 : 0.9406593406593406\n"
     ]
    }
   ],
   "source": [
    "# If k=5;\n",
    "\n",
    "knn_=KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\")\n",
    "\n",
    "knn_.fit(X_train,Y_train)\n",
    "\n",
    "print(\"Accuracy of test set for k=5 :\", knn_.score(X_test,Y_test))\n",
    "print(\"Accuracy of train set for k=5 :\", knn_.score(X_train,Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### k=5 için train set'in doğruluğu düşmüş olup, test set'in doğruluğu artmıştır; dolayısıyla k=5 için overfit olmuştur diyebiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameter: {'n_neighbors': 5}\n",
      "The best score: 0.9297619047619046\n"
     ]
    }
   ],
   "source": [
    "# To find the best k value, apply the GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parametreler={\"n_neighbors\":np.arange(1,8,1)}\n",
    "\n",
    "grid=GridSearchCV(estimator=knn,\n",
    "                 param_grid=parametreler,\n",
    "                 cv=10)\n",
    "\n",
    "grid.fit(X,Y)\n",
    "\n",
    "print(\"The best parameter:\", grid.best_params_)\n",
    "print(\"The best score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soru 4:\n",
    "Which one of the statements are true:\n",
    "* KNN performs much better if all of the data have the same scale\n",
    "* KNN works well with a small number of input variables (p), but struggles when the number of inputs is very large--True\n",
    "* KNN makes no assumptions about the functional form of the problem being solved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soru 5:\n",
    "Calculate the distance between A(1,3) and B(2,3) based on Euclidean and Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_euc=KNeighborsClassifier(n_neighbors=5, weights=\"distance\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance is= sqrt(square(2-1)+square(3-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclid=np.sqrt(np.square(2-1)+np.square(3-3))\n",
    "euclid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
